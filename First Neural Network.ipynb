{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow Net in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Build a Shallow neural network to classify MNIST digits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First steps (instructions for Mac or Linux). You need to install a recent version of Python, plus the packages keras, numpy, matplotlib and jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start with setting a seed to get reproducible code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the pre requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is a high level API for calling on the Tensorflow in the backend and is easy to start off.\n",
    "\n",
    "Keras is a (Batteries included) high-level neural network library that, among many other things, wraps an API similar to scikit-learn's around the Theano or TensorFlow backends.\n",
    "\n",
    "Keras is a high-level neural network library created by François Chollet at Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (7,7) # Make the figures a bit bigger\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,y_train),(X_test,y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train original shape\", X_train.shape) # handwritten images of digits\n",
    "print(\"y_train original shape\", y_train.shape) # actual value of those digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at some of the training data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(X_train[i], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Class {}\".format(y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets do some pre processing now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Flatten the input so that each 28x28 image becomes a single 784 dimensional vector.\n",
    "X_train = X_train.reshape(60000,784).astype('float32')\n",
    "X_test = X_test.reshape(10000,784).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change darkness from 0:255 to 0:1 \n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training matrix shape\", X_train.shape)\n",
    "print(\"Testing matrix shape\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras needs labels to be one hot encoded so we would modify the target matrices to be in the one-hot format, i.e.\n",
    "\n",
    "0 -> [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "1 -> [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "2 -> [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0] # it is no longer a two dimensional image but a one dimensional array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbr_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, nbr_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, nbr_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0] # 5 is one hot encoded here now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets start building our lego model aka neural network now "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the number of features/classes in your data provide constraints, you can determine all the other aspects of model structure: number of layers, size of layers, the nature of the connections between the layers, etc. (And if that didn't make sense, Keras is a great way to experiment with it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to define our model in the most common way: as a sequential stack of layers. \n",
    "# The alternative is as a computational graph, but we're going to stick to Sequential() here.\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(64, activation= 'sigmoid', input_shape = (784,))) \n",
    "# An \"activation\" is just a non-linear function applied to the output \n",
    "# of the layer above. Here, with a \"sigmoid\",\n",
    "# we clamp all values below 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(10,activation = 'softmax')) \n",
    "# This special \"softmax\" activation among other things,\n",
    "# ensures the output is a valid probaility distribution, that is\n",
    "# that its values are all non-negative and sum to 1.\n",
    "# the units of the hidden layer model an un normalized score of how likely the input\n",
    "# is to belong to a particular class.\n",
    "# Softmax layer normalizes this so that the output represents the probability for every class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets see what model did we create? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary() # we made a full connected dense network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "64*10+10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to tell this neuron how to learn\n",
    "\n",
    "Keras is built on top of Tensorflow,that allow you to define a computation graph in Python, which they then compile and run efficiently on the CPU or GPU without the overhead of the Python interpreter.\n",
    "\n",
    "When compiling a model, Keras asks you to specify your loss function and your optimizer. The loss function we'll start with is mean squared error and then we would check out categorical crossentropy, which is a loss function well-suited to comparing two probability distributions.\n",
    "\n",
    "Here our predictions are probability distributions across the ten different digits (e.g. \"we're 80% confident this image is a 3, 10% sure it's an 8, 5% it's a 2, etc.\"), and the target is a probability distribution with 100% for the correct category, and 0 for everything else. The cross-entropy is a measure of how different your predicted distribution is from the target distribution. More detail at Wikipedia\n",
    "\n",
    "The optimizer helps determine how quickly the model learns, how resistent it is to getting \"stuck\" or \"blowing up\". We won't discuss this in too much detail, but \"adam\" is often a good choice (developed here at U of T)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01),metrics = ['accuracy'])\n",
    "\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "#model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01),metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train,batch_size=128,epochs=20,verbose=1,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets evaluate the final performance, evaluate() returns the loss function and \n",
    "# any other metrics we asked for when we compiled the model. In our case, we asked for accuracy\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it does hint at one of the dangers of neural networks: overfitting. We've been careful here to hold out a test set and measure performance with that, but it's a small set, and 89% accuracy seems high to me, so I wouldn't be surprised if there was some overfitting going on. You could work on that by adding dropout (which is built into Keras). That's the neural network equivalent of the regularization our logistic regression classifier uses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
